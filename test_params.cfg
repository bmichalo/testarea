##############################################################################
#
# File:         test_params.cfg
#
# Description:  This file contains test parameters for programming the 
#               throughput tests as defined within the file 
#               'start-ovs-all-technologies.sh'
#
##############################################################################


#
# Identify location of OpenvSwitch executables
#
#prefix="/usr/local"    # Used with locally built src
prefix=""               # Used with RPMs


#
# Message mechanism used to manipulate the vring.
#
# cuse - User space character device and hook to a functional ioctl
#
# user - Create a Unix domain socket file through which messages are passed
#
# vhost can be user or cuse.  user is preferred as since DPDK 2.2 the majority
# of development has gone into enhancing vhost-user
#
#vhost="cuse" 
#vhost="user"
#
vhost="user"


#
# Adds DB_SOCK as a connection method used by ovsdb-server.
# connect to the Unix domain server socket named DB_SOCK.
# export DB_SOCK="$prefix/var/run/openvswitch/db.sock"
#
export DB_SOCK="$prefix/var/run/openvswitch/db.sock"


#
# Type of throughput test:
#
# Current OVS packet throughput testing comes in several different forms.  
# Test descriptions will include the following nomenclature:
#
# P = Physical interface
# V = Virtual interface
#
# Example 1:
#   {(PP)} ............. Bridge 0:  (Port 0 = 10G, Port 1 = 10G)
#
# Example 2:
#   {(PV),(VP)} ........ Bridge 0:  (Port 0 = 10G, Virtual Port 0 = virtio)
#                        Bridge 1:  (Virtual Port 1 = virtio, Port 1 = 10G)
#
# Example 3:
#   {(PV),(VV)(VP)} .... Bridge 0:  (Port 0 = 10G, Virtual Port 0 = virtio)
#                        Bridge 1:  (Virtual Port 1 = virtio, Virtual Port 2 = virtio)
#                        Bridge 2:  (Virtual Port 3 = virtio, Port 1 = 10G)
#
# Test Cases:
#
# I.    {(PP)} - Bridge 0:  (Port 0 = 10G, Port 1 = 10G)
#
#       where:
#
#       P_dataplane = kernel or dpdk
#       V_dataplane = none
#
#       This is 'bare metal' testing - no virtual machines
#
#       UNIDIRECTIONAL PACKET PATH:
#       ===========================
#           1.  Packets input to physical device P0
#
#           2.  OVS on host machine forwards packets from P0 to P1
#
#           3.  Packets go out physical device P1
#
#       Note:  BIDIRECTIONAL packet path includes the above and additionally
#              packets traversed into physical device P1,
#              through the DUT, and out device P0 (steps 3, 2, 1)
#
#
#
# II.  {(PV),(VP)} - Bridge 0:  (Port 0 = 10G, Virtual Port 0 = virtio)
#                    Bridge 1:  (Virtual Port 1 = virtio, Port 1 = 10G)
#
#       P_dataplane = kernel or dpdk
#       V_dataplane = kernel or dpdk
#
#       UNIDIRECTIONAL PACKET PATH:
#       ===========================
#           1.  Packets input to physical device P0
#
#           2.  Packets forwarded from P0 to V0 via host OVS bridge 0
#
#           3.  Packets in virtual machine will be forwarded from 
#               inteface V0 to interface V1.  Fowarding engines within
#               the guest can be 'testpmd' for DPDK, or 'brctl' for native
#               Linux kernel networking stack
#
#           4.  Packets forwarded from V1 to P1 via host OVS bridge 1
#
#           5.  Packets go out physical device P1
#
#       Note:  BIDIRECTIONAL packet path includes the above and additionally
#              packets traversed into physical device P1, through the DUT, \
#              and out device P0 (steps 5 back through 1)
#
#
#
# III.   {(PV),(VV),(VP)} - Bridge 1:  (Port 0 = 10G, Virtual Port 0 = virtio) 
#                           Bridge 2:  (Virtual Port 1 = virtio, Virtual Port 2 = virtio) 
#                           Bridge 3:  (Virtual Port 3 = virtio, Port 1 = 10G)
#
#       P_dataplane = kernel or dpdk
#       V_dataplane = kernel or dpdk
#
#       UNIDIRECTIONAL PACKET PATH:
#       ===========================
#           1.  Packets input to physical device P0
#
#           2.  Packets forwarded from P0 to V0 via host OVS bridge 0
#
#           3.  Packets in virtual machine will be forwarded from 
#               inteface V0 to interface V1.  Fowarding engines within
#               the guest can be 'testpmd' for DPDK, or 'brctl' for native
#               Linux kernel networking stack
#
#           4.  Packets forwarded from V1 to V2 via host OVS bridge 1
#
#           5.  Packets in virtual machine will be forwarded from 
#               inteface V2 to interface V3.  Fowarding engines within
#               the guest again can be 'testpmd' for DPDK, or 'brctl' for native
#               Linux kernel networking stack
#
#           6.  Packets forwarded from V3 to P1 via host OVS bridge 1
#
#           7.  Packets go out physical device P1
#
#       Note:  BIDIRECTIONAL packet path includes the above and additionally packets traversed into physical device B,
#              through the DUT, and out device A (steps 7 back through 1)
#
#
# Choose on the the following to set 'network_topology':
#
#network_topology="{(PP)}"
#network_topology="{(PV),(VP)}"
#network_topology="{(PV),(VV),(VP)}"
#
# Choose data plane types:
#
#P_dataplane=kernel
#P_dataplane=dpdk
#P_dataplane=none
#V_dataplane=kernel
#V_dataplane=dpdk
#V_dataplane=none
#
network_topology="{(VV)}"
P_dataplane=none
V_dataplane=dpdk


#
# For multiqueue feature, specify the number of 
# data queues per port.
#
num_queues_per_port=1


#
# This mask indicates how many DPDK PMD threads to create.  Note:  Take the
# number of ports:
#
# {(PP)} = 2 ports
# {(PV)(VP)} = 4 ports
# {(PV)(VV)(VP)} = 6 ports
#
# then multiply by number of queues per port to get optimum PMD thread count.
#
cpumask=3

#
# The network devices used in the test
#
dev1=p2p1
dev2=p2p2

#
# Path for DPDK tools (for dpdk_nic_bind.py)
#
dpdk_tools_path="/root/perf84/dpdk1607/dpdk-16.07/tools"

#
# Attempt interface driver binding.  1 = yes, 0 = no
#
bind_ifs=0
